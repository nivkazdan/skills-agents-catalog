# retellai-custom-llm

> Integrate custom LLM backends with Retell AI for specialized conversation handling

## Directory Structure

```
retellai-custom-llm/
├── SKILL.md
└── examples/
    └── example.py
```

## File Descriptions

| File | Type | Purpose |
|------|------|---------|
| SKILL.md | Markdown | Core skill instructions for custom LLM integration |
| examples/example.py | Python | Example custom LLM endpoint with Retell AI integration |

## Summary

**Category:** enterprise
**Target Audience:** ML engineers, Voice AI developers, Platform architects
**Trigger Phrases:** `retell custom LLM`, `retell bring your own LLM`, `retell LLM integration`, `retell custom model`

### What This Skill Does

This skill integrates custom LLM backends with Retell AI for specialized use cases. It covers setting up custom LLM endpoints that follow Retell's expected format, implementing response streaming for low latency, configuring fallback to default models, handling function calling and tool use, and optimizing for voice-specific response patterns.

### Technical Success Criteria

- Custom LLM endpoint deployed and accessible
- Response format validated against Retell requirements
- Streaming responses working correctly
- Fallback handling configured
- Latency within acceptable thresholds

### Business Success Criteria

- Differentiated AI capabilities for competitive advantage
- Specialized knowledge domains handled
- Custom LLM integration with <500ms additional latency

## Related Skills

- retellai-agent-creation - Connecting agents to custom LLM
- retellai-conversation-design - Optimizing prompts for custom LLM
- retellai-advanced-troubleshooting - Debugging custom LLM issues
