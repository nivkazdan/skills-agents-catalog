---
name: {{ company }}-incident-runbook
description: |
  Execute {{ display_name }} incident response procedures with triage, mitigation, and postmortem.
  Use when responding to {{ display_name }}-related outages, investigating errors,
  or running post-incident reviews for {{ display_name }} integration failures.
  Trigger with phrases like "{{ company }} incident", "{{ company }} outage",
  "{{ company }} down", "{{ company }} on-call", "{{ company }} emergency", "{{ company }} broken".
allowed-tools: Read, Grep, Bash(kubectl:*), Bash(curl:*)
version: 1.0.0
license: MIT
author: Jeremy Longshore <jeremy@intentsolutions.io>
---

# {{ display_name }} Incident Runbook

## Overview
Rapid incident response procedures for {{ display_name }}-related outages.

## Prerequisites
- Access to {{ display_name }} dashboard and status page
- kubectl access to production cluster
- Prometheus/Grafana access
- Communication channels (Slack, PagerDuty)

## Severity Levels

| Level | Definition | Response Time | Examples |
|-------|------------|---------------|----------|
| P1 | Complete outage | < 15 min | {{ display_name }} API unreachable |
| P2 | Degraded service | < 1 hour | High latency, partial failures |
| P3 | Minor impact | < 4 hours | Webhook delays, non-critical errors |
| P4 | No user impact | Next business day | Monitoring gaps |

## Quick Triage

```bash
# 1. Check {{ display_name }} status
curl -s {{ status_url | default('https://status.' + company + '.com/api/v2/status.json') }} | jq

# 2. Check our integration health
curl -s https://api.yourapp.com/health | jq '.services.{{ company }}'

# 3. Check error rate (last 5 min)
curl -s localhost:9090/api/v1/query?query=rate({{ company }}_errors_total[5m])

# 4. Recent error logs
kubectl logs -l app={{ company }}-integration --since=5m | grep -i error | tail -20
```

## Decision Tree

```
{{ display_name }} API returning errors?
â”œâ”€ YES: Is status.{{ company }}.com showing incident?
â”‚   â”œâ”€ YES â†’ Wait for {{ display_name }} to resolve. Enable fallback.
â”‚   â””â”€ NO â†’ Our integration issue. Check credentials, config.
â””â”€ NO: Is our service healthy?
    â”œâ”€ YES â†’ Likely resolved or intermittent. Monitor.
    â””â”€ NO â†’ Our infrastructure issue. Check pods, memory, network.
```

## Immediate Actions by Error Type

### 401/403 - Authentication
```bash
# Verify API key is set
kubectl get secret {{ company }}-secrets -o jsonpath='{.data.api-key}' | base64 -d

# Check if key was rotated
# â†’ Verify in {{ display_name }} dashboard

# Remediation: Update secret and restart pods
kubectl create secret generic {{ company }}-secrets --from-literal=api-key=NEW_KEY --dry-run=client -o yaml | kubectl apply -f -
kubectl rollout restart deployment/{{ company }}-integration
```

### 429 - Rate Limited
```bash
# Check rate limit headers
curl -v {{ api_url | default('https://api.' + company + '.com') }} 2>&1 | grep -i rate

# Enable request queuing
kubectl set env deployment/{{ company }}-integration RATE_LIMIT_MODE=queue

# Long-term: Contact {{ display_name }} for limit increase
```

### 500/503 - {{ display_name }} Errors
```bash
# Enable graceful degradation
kubectl set env deployment/{{ company }}-integration {{ company | upper }}_FALLBACK=true

# Notify users of degraded service
# Update status page

# Monitor {{ display_name }} status for resolution
```

## Communication Templates

### Internal (Slack)
```
ðŸ”´ P1 INCIDENT: {{ display_name }} Integration
Status: INVESTIGATING
Impact: [Describe user impact]
Current action: [What you're doing]
Next update: [Time]
Incident commander: @[name]
```

### External (Status Page)
```
{{ display_name }} Integration Issue

We're experiencing issues with our {{ display_name }} integration.
Some users may experience [specific impact].

We're actively investigating and will provide updates.

Last updated: [timestamp]
```

## Post-Incident

### Evidence Collection
```bash
# Generate debug bundle
./scripts/{{ company }}-debug-bundle.sh

# Export relevant logs
kubectl logs -l app={{ company }}-integration --since=1h > incident-logs.txt

# Capture metrics
curl "localhost:9090/api/v1/query_range?query={{ company }}_errors_total&start=2h" > metrics.json
```

### Postmortem Template
```markdown
## Incident: {{ display_name }} [Error Type]
**Date:** YYYY-MM-DD
**Duration:** X hours Y minutes
**Severity:** P[1-4]

### Summary
[1-2 sentence description]

### Timeline
- HH:MM - [Event]
- HH:MM - [Event]

### Root Cause
[Technical explanation]

### Impact
- Users affected: N
- Revenue impact: $X

### Action Items
- [ ] [Preventive measure] - Owner - Due date
```

## Instructions

### Step 1: Quick Triage
Run the triage commands to identify the issue source.

### Step 2: Follow Decision Tree
Determine if the issue is {{ display_name }}-side or internal.

### Step 3: Execute Immediate Actions
Apply the appropriate remediation for the error type.

### Step 4: Communicate Status
Update internal and external stakeholders.

## Output
- Issue identified and categorized
- Remediation applied
- Stakeholders notified
- Evidence collected for postmortem

## Error Handling
| Issue | Cause | Solution |
|-------|-------|----------|
| Can't reach status page | Network issue | Use mobile or VPN |
| kubectl fails | Auth expired | Re-authenticate |
| Metrics unavailable | Prometheus down | Check backup metrics |
| Secret rotation fails | Permission denied | Escalate to admin |

## Examples

### One-Line Health Check
```bash
curl -sf https://api.yourapp.com/health | jq '.services.{{ company }}.status' || echo "UNHEALTHY"
```

## Resources
- [{{ display_name }} Status Page]({{ status_url | default('https://status.' + company + '.com') }})
- [{{ display_name }} Support]({{ support_url | default('https://support.' + company + '.com') }})

## Next Steps
For data handling, see `{{ company }}-data-handling`.
